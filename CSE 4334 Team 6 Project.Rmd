---
title: "CSE 4334 Team 6 Project"
output: html_document
author: "Azmi Alinufael, Maheer Jawwad, Sujana Kabir"
date: "2025-11-07"
---

# Preparing Data

```{r}
# Load necessary libraries
# additional libraries can be added as needed

library(ggplot2)
library(caret)
library(dplyr)
library(corrplot)
library(GGally)
library(randomForest)


```

```{r}
#Loading and Reading Dataset

insure <- read.csv("Insurance.csv")

# Exploring the structure of the dataset
summary(insure)

# Checking for missing values
# looks clean, no missing values so far
sum(is.na(insure))

```

## Initial EDA
```{r}
# Converting categorical variables to factors
insure$sex <- as.factor(insure$sex)
insure$smoker <- as.factor(insure$smoker)
insure$region <- as.factor(insure$region)

#verifying the types
str(insure[,c("sex","smoker","region")])
```


```{r}
# Wanted to see which numeric variables are correlated.
# selecting only numeric columns first 
numeric_insure <- insure %>% select_if(is.numeric)
cor_matrix <- cor(numeric_insure)

# Plotting the heatmap
corrplot(cor_matrix, method = "color", addCoef.col = "black", type="upper", diag=FALSE)

```

It looks like age and bmi have some positive correlation with charges, but it's not super strong. We probably need to look at the categorical variables too.

```{r}
# Checking the distribution of our target variable, charges
ggplot(insure, aes(x = charges)) +
  geom_histogram(bins = 30, fill = "grey", color = "white") +
  labs(title = "Distribution of Insurance Charges") +
  theme_minimal()

```

The charges are heavily skewed to the right. Most people have lower charges, but there's a long tail of expensive claims. May have to fix this later (maybe log transformation) to make the regression work better.

# Baseline Linear Regression
note: haven't split the data into train/test yet.

```{r}
# Naive model
baseline_model <- lm(charges ~ ., data = insure)

summary(baseline_model)

```

R-squared around 0.75 means 75% of the insurance charge variance can be explained by this model. Also to note, all else being equal, being a smoker is associated with paying roughly \$24,421 more in charges than a non-smoker.


# Group Summaries

```{r}


# Basic numeric summaries
summary(insure)

# Mean charges by smoker status and gender
insure %>%
  group_by(smoker, sex) %>%
  summarise(mean_charges = mean(charges), .groups = "drop")

# Mean BMI and charges by region
insure %>%
  group_by(region) %>%
  summarise(avg_bmi = mean(bmi), avg_charge = mean(charges))


```
Looks like smokers pay way more on average, and the southeast region has the highest average charges.


# Univariate Visualizations

```{r}
# Histogram of charges
ggplot(insure, aes(x = charges)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Distribution of Insurance Charges", x = "Charges", y = "Frequency")

# Histogram of BMI
ggplot(insure, aes(x = bmi)) +
  geom_histogram(bins = 25, fill = "orange", color = "black") +
  labs(title = "Distribution of BMI", x = "BMI", y = "Count")

# Boxplot of Charges by Smoker Status
ggplot(insure, aes(x = smoker, y = charges, fill = smoker)) +
  geom_boxplot() +
  labs(title = "Charges by Smoking Status", y = "Insurance Charges")

```
Smokers have a much wider range of charges, with many outliers on the high end.

# Bivariate Relationships
```{r}
# Scatter: Age vs Charges
ggplot(insure, aes(x = age, y = charges, color = smoker)) +
  geom_point(alpha=0.6) +
  labs(title = "Age vs Charges (Smoker Highlighted)")

# Scatter: BMI vs Charges
ggplot(insure, aes(x = bmi, y = charges, color = smoker)) +
  geom_point(alpha=0.6) +
  labs(title = "BMI vs Charges (Smoker Highlighted)")

```

Charges rise with both age and BMI, especially for smokers.

# Outlier Detection

```{r}
# Outliers in charges
ggplot(insure, aes(y = charges)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Boxplot of Insurance Charges")

# Outliers in BMI
ggplot(insure, aes(y = bmi)) +
  geom_boxplot(fill = "lightcoral") +
  labs(title = "Boxplot of BMI")

```
There are definitely some high-value outliers in charges, likely from smokers with health issues. BMI also has a few outliers but not as extreme.



# EDA Summary

The dataset includes 1338 observations and 8 variables: age, sex, bmi, children, smoker, region, charges, and insuranceclaim. No missing values were found. Categorical variables (sex, smoker, and region) were converted into factors for analysis. Our target variable (charges) is heavily skewed to the right, with a small number of high cost outliers corresponding to expensive medical claims. The descriptive stats and visuals show that smokers consistently incur higher medical charges compared to non-smokers, and that both age and BMI have moderate positive relationships with charges.The variable insuranceclaim serves as a potential target for later classification task but is excluded for out regression modeling of charges to avoid leakage. Overall, smoking, status, BMI, and age are expected to be the most significant predictors of individual medical costs.



# Part 2: Building Model

# Modeling and Evaluation

```{r}
set.seed(123)

# Splitting the data into training and testing sets (70-30 split)
train_index <- createDataPartition(insure$charges, p = 0.7, list = FALSE)

train_data <- insure[train_index, ]
test_data <- insure[-train_index, ]

#verifying the split
dim(train_data)
dim(test_data)

```
# Multiple Linear Regression

```{r}
# Building the multiple linear regression model

mlr_model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = train_data)

# model summary
summary(mlr_model)

# Predicting on the test set
pred_mlr <- predict(mlr_model, newdata = test_data)

# Evaluating with RMSE and R^2
rmse_mlr <- sqrt(mean((test_data$charges - pred_mlr)^2))
r2_mlr <- cor(test_data$charges, pred_mlr)^2

cat("Multiple Linear Regression RMSE:", rmse_mlr, "\n")
cat("Multiple Linear Regression R-squared:", r2_mlr, "\n")

```


# Random Forest Regression

```{r}
# Building the random forest model
set.seed(123)

rf_model <- randomForest(charges ~ age + sex + bmi + children + smoker + region, data = train_data, ntree = 500, importance = TRUE)


# Predicting on the test set
pred_rf <- predict(rf_model, newdata = test_data)

# Evaluating with RMSE and R^2
rmse_rf <- sqrt(mean((test_data$charges - pred_rf)^2))
r2_rf <- cor(test_data$charges, pred_rf)^2

cat("Random Forest RMSE:", rmse_rf, "\n")
cat("Random Forest R-squared:", r2_rf, "\n")

```
# Comparing Models

```{r}
model_comparison <- data.frame(
  Model = c("Multiple Linear Regression", "Random Forest"),
  RMSE = c(rmse_mlr, rmse_rf),
  R_squared = c(r2_mlr, r2_rf)
)
print(model_comparison)
```
Both models worked well but Random Forest achieved a lower RMSE and Higher R^2 value


# Feature Importance
```{r}
importance(rf_model)
varImpPlot(rf_model, main = "Variable Importance from Random Forest Model")
```
It seems that smoker, age, and bmi are the top predictors of insurance charges according to the random forest model. The region, sex, and children showed less importance comparatively.


# Residual Analysis
```{r}
residuals_mlr <- data.frame(
  Predicted = pred_mlr,
  Residuals = test_data$charges - pred_mlr
)

ggplot(residuals_mlr, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha=0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Predicted (MLR Model)", x = "Predicted Charges", y = "Residuals")

```

# Modeling Summary

We used two regression models to train and predict individual medical costs (charges): Multiple Linear Regression and Random Forest Regression. Both models performed well, but Random Forest achieved a lower MSE and higher R^2 value, showing a strong predictive accuracy and the handleing of non-linear relationships.

Across both models, the most predictors that influence the model the most were smoker, bmi, and age. As we expected that a bad life style choice were associated with the large increase in predicted medical charges, while also bad bmi and old age also contributed to the total costs.

Overall, the results show that lifestyle and demographic factors such as being a smoker and bad bmi play a role in determining healthcare costs.


# Optional: Classification

```{r}
# Check class balance
table(insure$insuranceclaim)

# Convert insuranceclaim to factor if not already
insure$insuranceclaim <- as.factor(insure$insuranceclaim)

# Split into train/test (same 70/30 ratio)
set.seed(123)
train_index <- createDataPartition(insure$insuranceclaim, p = 0.7, list = FALSE)
train_class <- insure[train_index, ]
test_class  <- insure[-train_index, ]

```
# Logistic Regression Model
```{r}
# Logistic Regression Model
log_model <- glm(insuranceclaim ~ age + sex + bmi + children + smoker + region,
                 data = train_class, family = "binomial")

summary(log_model)

# Predictions (probabilities)
pred_log_prob <- predict(log_model, newdata = test_class, type = "response")

# Convert to binary (threshold = 0.5)
pred_log <- ifelse(pred_log_prob > 0.5, 1, 0)
pred_log <- as.factor(pred_log)
# Confusion Matrix
confusion_log <- confusionMatrix(pred_log, test_class$insuranceclaim)
print(confusion_log)

```
# Random Forest Classification Model
```{r}
set.seed(123)
rf_class <- randomForest(insuranceclaim ~ age + sex + bmi + children + smoker + region,
                         data = train_class, ntree = 500, importance = TRUE)

# Predict
pred_rf <- predict(rf_class, newdata = test_class)
# Confusion Matrix
confusion_rf <- confusionMatrix(pred_rf, test_class$insuranceclaim)
print(confusion_rf)
```
# Model Evaluation Summary

```{r}
library(caret)

# Logistic Regression Evaluation
confusionMatrix(pred_log, test_class$insuranceclaim, positive = "1")

# Random Forest Evaluation
confusionMatrix(pred_rf, test_class$insuranceclaim, positive = "1")

```
# ROC Curve and AUC score
```{r}
library(pROC)

# ROC for Logistic Regression
roc_log <- roc(test_class$insuranceclaim, pred_log_prob)
plot(roc_log, col = "blue", main = "ROC Curve (Logistic Regression)")
auc(roc_log)

# ROC for Random Forest
roc_rf <- roc(test_class$insuranceclaim, as.numeric(pred_rf))
plot(roc_rf, col = "red", main = "ROC Curve (Random Forest)")
auc(roc_rf)


```
# Confusion Matrix Plots
```{r}
cm_rf <- confusionMatrix(pred_rf, test_class$insuranceclaim)
cm_df <- as.data.frame(cm_rf$table)

ggplot(cm_df, aes(Prediction, Reference, fill = Freq)) +
  geom_tile() + geom_text(aes(label = Freq), color = "white", size = 5) +
  scale_fill_gradient(low = "steelblue", high = "darkred") +
  labs(title = "Random Forest Confusion Matrix", x = "Predicted", y = "Actual") +
  theme_minimal()

```

